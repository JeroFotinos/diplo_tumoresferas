{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S_T19aNnGhb"
   },
   "source": [
    "\n",
    "<center>\n",
    "<h4>Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación</h4>\n",
    "<h3>Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</h3>\n",
    " <h2>Mentoría: Clasificación de Tumoresferas </h2>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCZIyaVatb3o"
   },
   "source": [
    "<a name=\"exploratory_data_analysis\"></a>\n",
    "# **Práctico de Aprendizaje Supervisado**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIXIZi43XKcC"
   },
   "source": [
    "Importamos las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kdtKG4W2XL1d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set_context('talk')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k05k3tQXoo_q"
   },
   "source": [
    "En este práctico, utilizaremos el archivo original *fiji_datos_0al7mo_labels.csv* que se encuentra en la carpeta *data/raw/*.\n",
    "\n",
    "Además sumaremos los siguientes datos sintéticos que se encuentran en la carpeta *data/datos_sinteticos/*:\n",
    "  * datos_sinteticos_dias_3_y_5.csv\n",
    "  * synthetic_3y5_sint2.csv\n",
    "  * synthetic_data_dia_3_si.csv\n",
    "  * synthetic_data_dia_4_si.csv\n",
    "  * synthetic_data_dia_5_si.csv\n",
    "\n",
    "Como también dos archivos extras que están en la capeta *data/03_AS/*:\n",
    "  * fiji_datos_mean_diam.csv\n",
    "  * fiji_datos_noise.csv\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Lr0nJQnpBVT"
   },
   "source": [
    "## Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9tpZZcxpJsa"
   },
   "source": [
    "$1.$   Utilizando del día 1 al 5 los datos clasificados como 'Esferoide' = 'si', realizar un ajuste del diámetro medio. Sean los datos reales *mean_diam_df* y la señal ruidosa *df_noise*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "n3PDpq_U8DhF",
    "outputId": "50fb2660-aaa9-4721-f844-a2209d28ddb2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/03_AS/fiji_datos_mean_diam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mean_diam_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/03_AS/fiji_datos_mean_diam.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m mean_diam_df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/03_AS/fiji_datos_mean_diam.csv'"
     ]
    }
   ],
   "source": [
    "mean_diam_df = pd.read_csv(\"data/03_AS/fiji_datos_mean_diam.csv\")\n",
    "mean_diam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkjZrs2Z97Pm"
   },
   "outputs": [],
   "source": [
    "df_noise = pd.read_csv(\"data/03_AS/fiji_datos_noise.csv\")\n",
    "#df_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "FNUsPK9z-IpL",
    "outputId": "951950d0-97a9-472d-9daf-e560588880a4"
   },
   "outputs": [],
   "source": [
    "X_noise = df_noise['dia']\n",
    "y_noise = df_noise['mean']\n",
    "X_noise = np.array(X_noise.to_list())\n",
    "y_noise = np.array(y_noise.to_list())\n",
    "\n",
    "X_mean = mean_diam_df['dia']\n",
    "y_mean = mean_diam_df['mean']\n",
    "X_mean = np.array(X_mean.to_list())\n",
    "y_mean = np.array(y_mean.to_list())\n",
    "\n",
    "\n",
    "plt.scatter(X_noise, y_noise, color=\"blue\", label=\"noise\")\n",
    "plt.scatter(X_mean, y_mean, color=\"yellow\", label=\"mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EU5e4bKAFFk"
   },
   "source": [
    "Probar un ajuste lineal, polinomial, probar el ajuste \"óptimo\" que da el menor error cuadrático y sobreajuste.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis para mean_diam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las características y la variable objetivo\n",
    "X = mean_diam_df[['dia']]  \n",
    "y = mean_diam_df['mean']\n",
    "\n",
    "# Creamos un modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo con los datos\n",
    "model.fit(X, y)\n",
    "\n",
    "# Realizamos predicciones con el modelo\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "plt.scatter(X, y, color='gold', label='Datos reales')\n",
    "plt.plot(X, y_pred, color='darkorange', linewidth=2, label='Regresión lineal')\n",
    "plt.xlabel('Día')\n",
    "plt.ylabel('Diámetro medio')\n",
    "plt.legend(fontsize='small', loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "coeficiente_pendiente = round(model.coef_[0], 1)\n",
    "termino_independiente = round(model.intercept_, 1)\n",
    "\n",
    "print(\"Pendiente:\", coeficiente_pendiente)\n",
    "print(\"Término independiente:\", termino_independiente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos las predicciones del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculamos el error cuadrático medio\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mse_rounded = round(mse, 1)\n",
    "\n",
    "print(\"Error cuadrático medio:\", mse_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "\n",
    "degrees = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for degree in degrees:\n",
    "    # Entreno al modelo para cada uno de los grados en degrees:\n",
    "    pf = PolynomialFeatures(degree)\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    model = make_pipeline(pf, lr)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # predigo los valores con el modelo:\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # evaluo el error:\n",
    "    train_error = round(mean_squared_error(y, y_pred),2)\n",
    "\n",
    "    # Armo el array con los errores\n",
    "    train_errors.append(train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el MSE en función del grado del polinomio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree, train_error in zip(degrees, train_errors):\n",
    "    print(degree, train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_errors, color=\"gold\")\n",
    "plt.xlim(0,11)\n",
    "plt.ylim(0,1200)\n",
    "plt.xlabel(\"Grado de Polinomio\", size= 15)\n",
    "plt.ylabel(\"MSE\", size= 15)\n",
    "plt.title(\"Error Cuadrático Medio (MSE) vs. Grado de Polinomio\", pad=20, size= 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mse disminuye abruptamente a partir del grado 1 al 5, dando indicios de sobreajuste a partir del grado 5, donde el MSE es 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontramos el índice del grado de polinomio con el menor error en el conjunto de evaluación\n",
    "best_degree_index = np.argmin(train_errors)\n",
    "best_degree = degrees[best_degree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos los errores cuadráticos medios para el mejor grado de polinomio\n",
    "print(\"Mejor grado de polinomio:\", best_degree)\n",
    "print(\"MSE:\", train_errors[best_degree_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como podemos ver en la tabla de MSE (errores cuadráticos medios) como gráficamente, el grado del polinomio que reduce los errores y mejor ajusta los datos, es el polinomio de grado 5. El ajuste mejora considerablemente respecto a la regresión lineal dado que se encontraron MSE de menor valor que para la regresión lineal.\n",
    "\n",
    "No obstante, podríamos pensar que la regresión lineal es un mejor modelo para aplicar a nuestros datos. Cuando el MSE es nulo, se origina un sobreajuste de nuestros datos. Esto ocurre a partir del grado 5 en la regresión polinomial. Ahora bien, el MSE para el polinomio de grado 4 tiene un valor de 26,9 (sería el MSE de menor valor sin sobreajuste), mientras que el MSE de la regresión lineal es 15,9. Teniendo esto en consideración, se podría seleccionar como mejor modelo el de regresión lineal para el ajuste de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis para df_noise\n",
    "\n",
    "### Regresión lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos las características y la variable objetivo\n",
    "X = df_noise[['dia']]  \n",
    "y = df_noise['mean']\n",
    "\n",
    "# Creamos un modelo de regresión lineal\n",
    "model = LinearRegression()\n",
    "\n",
    "# Entrenamos el modelo con los datos\n",
    "model.fit(X, y)\n",
    "\n",
    "# Realizamos predicciones con el modelo\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Visualizamos los resultados\n",
    "plt.scatter(X, y, color='gold', label='Datos reales')\n",
    "plt.plot(X, y_pred, color='darkorange', linewidth=2, label='Regresión lineal')\n",
    "plt.xlabel('Día')\n",
    "plt.ylabel('Diámetro medio')\n",
    "plt.legend(fontsize='small', loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "coeficiente_pendiente = round(model.coef_[0], 1)\n",
    "termino_independiente = round(model.intercept_, 1)\n",
    "\n",
    "print(\"Pendiente:\", coeficiente_pendiente)\n",
    "print(\"Término independiente:\", termino_independiente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos las predicciones del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculamos el error cuadrático medio\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "mse_rounded = round(mse, 1)\n",
    "\n",
    "print(\"Error cuadrático medio:\", mse_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos ruidosos originan que el ajuste del modelo empeore, por lo que el MSE aumenta considerablemente para la regresión lineal con respecto a los datos sin ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "\n",
    "degrees = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for degree in degrees:\n",
    "    # Entreno al modelo para cada uno de los grados en degrees:\n",
    "    pf = PolynomialFeatures(degree)\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    model = make_pipeline(pf, lr)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # predigo los valores con el modelo:\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # evaluo el error:\n",
    "    train_error = round(mean_squared_error(y, y_pred),2)\n",
    "\n",
    "    # Armo el array con los errores\n",
    "    train_errors.append(train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los MSE para cada grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree, train_error in zip(degrees, train_errors):\n",
    "    print(degree, train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_errors, color=\"gold\")\n",
    "plt.xlim(0,11)\n",
    "plt.ylim(0,1200)\n",
    "plt.xlabel(\"Grado de Polinomio\", size= 15)\n",
    "plt.ylabel(\"MSE\", size= 15)\n",
    "plt.title(\"Error Cuadrático Medio (MSE) vs. Grado de Polinomio\", pad=20, size= 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso podemos observar que los MSE para los grados del 0 al 5 son un poco menores que para los datos sin ruido. No obstante, no se observa que el MSE se haga cero para los polinomios de mayor grado como sí sucede para aquellos datos sin ruido. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en Entrenamiento y Evaluación\n",
    "Dividimos aleatoriamente los datos en 80% para entrenamiento y 20% para evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos primero el caso de los datos no ruidosos.\n",
    "X = mean_diam_df['dia'].values.reshape(-1, 1)\n",
    "y = mean_diam_df['mean'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "print(\"Dimensiones de X_train:\", X_train.shape)\n",
    "print(\"Dimensiones de X_test:\", X_test.shape)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGEKW8_oAkrQ"
   },
   "source": [
    "## Clasificación\n",
    "\n",
    "Aquí vamos a usar el dataset sin modificar, que se encuentra en 'raw/fiji_datos_0al7mo_labels.csv', en la carpeta del [repositorio](https://github.com/luciabarg/datos_tumoresferas/tree/main/data/datos_sinteticos) y de la [carpeta compartida](https://drive.google.com/drive/folders/1RqGNySwACN33Qopmw0nHmj5Yv4M78ZXi?usp=drive_link).\n",
    "\n",
    "1) Teniendo en cuenta la naturaleza de nuestros datos, es adecuado plantearlo como un problema supervisado? De qué tipo? Justificar.\n",
    "   \n",
    "2) El dataset que tenemos, es linealmente separable?\n",
    "\n",
    "3) Explorar las características de los datos sintéticos generados, comparar con los datos que tenemos de los días 3,4 y 5. Los datos se encuentran en la carpeta del [repositorio](https://github.com/luciabarg/datos_tumoresferas/tree/main/data/datos_sinteticos) y de la [carpeta compartida](https://drive.google.com/drive/folders/1RqGNySwACN33Qopmw0nHmj5Yv4M78ZXi?usp=drive_link):\n",
    "\n",
    "4) Elegir uno de las siguientes situaciones y generar su correspondiente dataset (leer hasta el final del enunciado antes de generarlos):\n",
    "\n",
    "  * Tomar todo el dataset.\n",
    "\n",
    "  * Realizar clasificación con solo los datos del día 3, 4 y 5.\n",
    "\n",
    "  * Tomar solo los días 3,4 y 5 y sumar los datos sintéticos.\n",
    "  \n",
    "  * Utilizando sólo PCA (si quieren de todos días o solo de los días 3,4 y 5, con o sin datos sintéticos) pero indicar cuál se tomó.\n",
    "\n",
    "\\\n",
    "**Elegir un escenario**, recordar eliminar una de las columnas altamente correlacionadas y también aquellas que no suman al problema. Si hubiera una columna constante, también deberían excluirla. Separen en conjuntos de entrenamiento y test, hacer tratamiento de ouliers, agregar columnas preprocesadas al original como algunas que sean combinaciones/modificaciones de las demás, multiplicaciones, logaritmos, potencias, [por ej](https://docs.google.com/presentation/d/e/2PACX-1vSLfKgsq-NuF2aWQF3OPkgLvBn25A2khGh0QIJkpFb6QgRZ7dGne32GEkTiC4M6yg/pub?start=false&loop=false&delayms=3000&slide=id.gb81ac3e375_0_32), verificando que no sean correlacionadas con las que ya tienen.\n",
    "\n",
    "\n",
    "**Recordar que las transformaciones se realizan sobre el conjunto de train y luego se ajusta el de test**. Si hacen todo antes y luego separan, puede haber traspaso de información sobre el conjunto de train al test (*data leakage*).\n",
    "\n",
    "\n",
    "Realizar un EDA rápido de como quedaron las variables y si la distribuciones en test son similares a las que tienen en train.\n",
    "En todos los casos, tanto para train y test, visualizar la cantidad de datos para cada clase y calcular el porcentaje de las mismas.\n",
    "\n",
    "Tenemos pocos datos, es muy posible que se genere overfitting. Cómo podrían tratar de solventar este inconveniente? Implementar si es posible.\n",
    "\n",
    "Implementar sobre los conjuntos *test* y *train* del escenario elegido algún clasificador lineal como también probar DT, Random Forest y XGBoost.\n",
    "\n",
    "Explorar con parámetros de defecto (modelo baseline) y con búsqueda de hiperparámetros y por medio de las diferentes métricas (sobre todo f1 y precision) determinar cuál es el mejor algoritmo de clasificación.\n",
    "En los algoritmos que lo permitan, hacer listado de importancia de features y probar con diferentes combinaciones de columnas si la métrica hallada mejora si se disminuye la cantidad de columnas (puede ser que esto no ocurra también).\n",
    "\n",
    "Con el algoritmo que tenga las mejores métricas , probar si mejora o empeoran la clasificación probando algún otro escenario (con todos los pasos que implica).\n",
    "\n",
    "\\\n",
    "OPCIONAL:\n",
    "\n",
    "\n",
    "Probar la métrica: coeficiente de correlación de [Matthews](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7) (está implementado en sklearn como una métrica más: *from sklearn.metrics import matthews_corrcoef* )\n",
    "\n",
    "Si se animan, tenemos 5 días. Probar clasificación multiclase para identificar a q día se corresponden las muestras. O tomar solo los 3 que usaron en los escenarios anteriores.\n",
    "\n",
    "Super buenas prácticas en este [repositorio](https://github.com/daianadte/wids-cba-2023/), es muy instructivo chusmear sobre todo el archivo  ['06_FinalModel.ipynb'](https://github.com/daianadte/wids-cba-2023/blob/main/06_FinalModel.ipynb) solo para que vean un análisis posterior a implementar los modelos y que existen los Shap Values, que es una técnica utilizada para explicar las predicciones.\n",
    "\n",
    "\n",
    "----\n",
    "Como este es el último práctico, si se animan, podrían implementar clústering.\n",
    "Pueden implementar kmeans sobre el dataset normalizado, usando PCA, o incluso animarse a probar t-SNE y UMAP.\n",
    "\n",
    "Realizar EDA sobre los conjuntos que encontraron y traten de explicarlos.\n",
    "\n",
    "Muchas veces en problemas de negocios, no es tan importante el modelo en sí, qué tan bien separa los datos sino la interpretabilidad que se puede dar a los resultados. Podríamos coordinar con Luciano para charlar si lo que encontraron a partir de las agrupaciones, es coherente o no, sería como parte de charlar con el \"cliente\" a ver si está de acuerdo a lo hallado. O sea, pueden encontrar explicaciones a los datos distintos que la de \"esferoides\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juYPQ-ASs95F"
   },
   "source": [
    "### $1.$ Teniendo en cuenta la naturaleza de nuestros datos, es adecuado plantearlo como un problema supervisado? De qué tipo? Justificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestros datos se adecuan a un problema de aprendizaje supervisado debido a que contamos con etiquetas previamente asignados a cada instancia de datos. En este caso, nuestra variable objetivo 'esferoide' es binaria, tomando solo dos valores posibles: \"sí\" o \"no\". Esto indica que estamos abordando un problema de clasificación binaria.\n",
    "\n",
    "El aprendizaje supervisado implica que nuestro modelo aprenderá a partir de ejemplos de entrada y sus correspondientes etiquetas de salida, con el objetivo de realizar predicciones precisas sobre nuevas instancias de datos no etiquetados. En este contexto, estamos entrenando un modelo para predecir si una instancia pertenece a una de las dos clases: \"sí\" o \"no\", lo que confirma la naturaleza de clasificación binaria de nuestro problema.\n",
    "\n",
    "En resumen, nuestro problema se ajusta a un problema de aprendizaje supervisado de clasificación binaria, ya que contamos con etiquetas y el objetivo es predecir la clase de nuevas instancias en una de las dos categorías posibles: \"sí\" o \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juYPQ-ASs95F"
   },
   "source": [
    "### $2.$ El dataset que tenemos, es linealmente separable?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levantamos el archivo\n",
    "url = 'https://raw.githubusercontent.com/luciabarg/datos_tumoresferas/main/data/raw/fiji_datos_0al7mo_labels.csv'\n",
    "df_fiji_datos = pd.read_csv(url)\n",
    "df_fiji_datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitamos el análisis de PCA realizado para el tp anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Standarization\n",
    "cols_to_project = ['Area', \n",
    "                         'Perim.',\n",
    "                         #'Width',\n",
    "                         #'Height',\n",
    "                         'Circ.',\n",
    "                         'Feret',\n",
    "                         'MinFeret',\n",
    "                         'AR',\n",
    "                         'Round',\n",
    "                         'Solidity',\n",
    "                         #'Esferoide',\n",
    "                         'Diameter',\n",
    "                         'n_diam']\n",
    "\n",
    "df_standard = df_fiji_datos[cols_to_project].copy()\n",
    "\n",
    "# Standardize the data\n",
    "df_standard = (df_standard - df_standard.mean()) / df_standard.std()\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(df_standard)\n",
    "\n",
    "df_projected = pca.transform(df_standard)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "g = sns.JointGrid(x=df_projected[:, 0], y=df_projected[:, 1],hue=df_fiji_datos['Esferoide'], height=6)\n",
    "g.plot_joint(sns.scatterplot, s=100)\n",
    "g.plot_marginals(sns.stripplot)\n",
    "plt.tight_layout()  # Añadir esta línea para ajustar el gráfico\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del gráfico de las dos componentes principales de PCA podríamos decir que a simple vista los datos parecerían ser linealmente separables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juYPQ-ASs95F"
   },
   "source": [
    "### $3.$ Explorar las características de los datos sintéticos generados, comparar con los datos que tenemos de los días 3,4 y 5. Los datos se encuentran en la carpeta del repositorio y de la carpeta compartida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juYPQ-ASs95F"
   },
   "source": [
    "$4.$ Elegir uno de las siguientes situaciones y generar su correspondiente dataset (leer hasta el final del enunciado antes de generarlos):\n",
    "\n",
    "* Tomar todo el dataset.\n",
    "\n",
    "* Realizar clasificación con solo los datos del día 3, 4 y 5.\n",
    "\n",
    "* Tomar solo los días 3,4 y 5 y sumar los datos sintéticos.\n",
    "  \n",
    "* Utilizando sólo PCA (si quieren de todos días o solo de los días 3,4 y 5, con o sin datos sintéticos) pero indicar cuál se tomó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
